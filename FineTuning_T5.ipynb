{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubLEqxvzpxjB",
        "outputId": "5140bd77-f188-4bbc-c224-e3b7c52fe48c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "BwUNspCoh4nI",
        "outputId": "05a0e0eb-ade0-4976-e9c8-8617ffc94067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from samanantar_4950k_filtered.tsv...\n",
            "Loaded 4891173 translation pairs.\n",
            "Training set: 4402055 examples\n",
            "Validation set: 489118 examples\n",
            "Loading T5 tokenizer and model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-d0064747a118>:199: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CycleConsistencyTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5727' max='825387' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5727/825387 1:23:30 < 199:14:52, 1.14 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5998' max='825387' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5998/825387 1:27:27 < 199:11:39, 1.14 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Dict, List, Optional\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Download NLTK data locally (one-time operation)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration - CHANGE THESE VARIABLES\n",
        "TSV_FILE_PATH = \"samanantar_4950k_filtered.tsv\"  # REPLACE WITH YOUR FILE PATH\n",
        "OUTPUT_DIR = \"t5_khasi_english_model\"\n",
        "MAX_SOURCE_LENGTH = 128\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 3\n",
        "WARMUP_STEPS = 500\n",
        "WEIGHT_DECAY = 0.01\n",
        "# Determine if the dataset is Khasi->English or English->Khasi\n",
        "IS_KHASI_TO_ENGLISH = True  # CHANGE THIS BASED ON YOUR DATASET DIRECTION\n",
        "\n",
        "# Define markers for language indication\n",
        "SRC_PREFIX = \"translate Khasi to English: \" if IS_KHASI_TO_ENGLISH else \"translate English to Khasi: \"\n",
        "TGT_PREFIX = \"translate English to Khasi: \" if IS_KHASI_TO_ENGLISH else \"translate Khasi to English: \"\n",
        "\n",
        "# Load the dataset\n",
        "def load_dataset(tsv_file_path):\n",
        "    # The file has \\t \\t \\t \\t \\t as the end marker\n",
        "    # Assuming the format is source \\t target \\t \\t \\t \\t \\t\n",
        "    with open(tsv_file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    sources = []\n",
        "    targets = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Replace the end marker\n",
        "        line = line.replace(\"\\t \\t \\t \\t \\t\", \"\")\n",
        "        # Split on tab\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            sources.append(parts[0].strip())\n",
        "            targets.append(parts[1].strip())\n",
        "\n",
        "    return pd.DataFrame({'source': sources, 'target': targets})\n",
        "\n",
        "# Calculate BLEU score using NLTK (no external API)\n",
        "def calculate_bleu(reference, hypothesis):\n",
        "    smoother = SmoothingFunction().method1\n",
        "\n",
        "    # Tokenize sentences (split into words)\n",
        "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
        "\n",
        "    # Calculate BLEU score (with smoothing for short sentences)\n",
        "    try:\n",
        "        return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoother) * 100\n",
        "    except Exception:\n",
        "        return 0.0  # Return 0 if calculation fails\n",
        "\n",
        "# Translation Dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_source_length, max_target_length, source_prefix, target_prefix=None):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "        self.source_prefix = source_prefix\n",
        "        self.target_prefix = target_prefix  # For cycle consistency\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = self.data.iloc[idx]['source']\n",
        "        target = self.data.iloc[idx]['target']\n",
        "\n",
        "        source_text = self.source_prefix + source\n",
        "\n",
        "        # Tokenize inputs\n",
        "        source_encoding = self.tokenizer(\n",
        "            source_text,\n",
        "            max_length=self.max_source_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Tokenize targets\n",
        "        target_encoding = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Create input_ids and attention_mask\n",
        "        input_ids = source_encoding.input_ids.squeeze()\n",
        "        attention_mask = source_encoding.attention_mask.squeeze()\n",
        "        labels = target_encoding.input_ids.squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100  # Replace pad tokens\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"original_source\": source,  # Store original for cycle consistency\n",
        "            \"original_target\": target   # Store original for cycle consistency\n",
        "        }\n",
        "\n",
        "# Compute BLEU Score (no external API)\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # Replace -100 with pad token id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate BLEU scores for each prediction-reference pair\n",
        "    bleu_scores = []\n",
        "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
        "        bleu_score = calculate_bleu(ref, pred)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Average BLEU score\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "\n",
        "    return {\"bleu\": avg_bleu}\n",
        "\n",
        "# Cycle Consistency Loss Calculation\n",
        "def compute_cycle_loss(model, tokenizer, batch, device):\n",
        "    # Forward pass: source -> target\n",
        "    src_texts = [SRC_PREFIX + text for text in batch[\"original_source\"]]\n",
        "    tgt_texts = batch[\"original_target\"]\n",
        "\n",
        "    # Source -> Target\n",
        "    src_encodings = tokenizer(src_texts, padding=True, truncation=True,\n",
        "                             max_length=MAX_SOURCE_LENGTH, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=src_encodings.input_ids,\n",
        "            attention_mask=src_encodings.attention_mask,\n",
        "            max_length=MAX_TARGET_LENGTH\n",
        "        )\n",
        "\n",
        "    # Decode the generated targets\n",
        "    generated_tgts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Target -> Source (cycle back)\n",
        "    inverse_texts = [TGT_PREFIX + text for text in generated_tgts]\n",
        "    inverse_encodings = tokenizer(inverse_texts, padding=True, truncation=True,\n",
        "                                 max_length=MAX_SOURCE_LENGTH, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cycle_outputs = model.generate(\n",
        "            input_ids=inverse_encodings.input_ids,\n",
        "            attention_mask=inverse_encodings.attention_mask,\n",
        "            max_length=MAX_SOURCE_LENGTH\n",
        "        )\n",
        "\n",
        "    # Decode the reconstructed sources\n",
        "    reconstructed_srcs = tokenizer.batch_decode(cycle_outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate BLEU between original sources and reconstructed sources\n",
        "    cycle_scores = []\n",
        "    for orig_src, recon_src in zip(batch[\"original_source\"], reconstructed_srcs):\n",
        "        bleu = calculate_bleu(orig_src, recon_src)\n",
        "        cycle_scores.append(bleu)\n",
        "\n",
        "    return sum(cycle_scores) / len(cycle_scores) if cycle_scores else 0\n",
        "\n",
        "# Custom Trainer with Cycle Loss\n",
        "class CycleConsistencyTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, model=None, args=None, data_collator=None, train_dataset=None,\n",
        "                 eval_dataset=None, tokenizer=None, compute_metrics=None, **kwargs):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        self.cycle_losses = []\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
        "        metrics = super().evaluate(\n",
        "            eval_dataset=eval_dataset,\n",
        "            ignore_keys=ignore_keys,\n",
        "            metric_key_prefix=metric_key_prefix\n",
        "        )\n",
        "\n",
        "        # Calculate cycle loss on evaluation dataset\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        model = self.model.to(self.args.device)\n",
        "        model.eval()\n",
        "\n",
        "        # Process batches for cycle loss\n",
        "        cycle_scores = []\n",
        "        for batch in eval_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(self.args.device) if isinstance(v, torch.Tensor) else v\n",
        "                     for k, v in batch.items()}\n",
        "\n",
        "            # Extract original text data\n",
        "            orig_sources = []\n",
        "            orig_targets = []\n",
        "\n",
        "            # Try to get original source/target if available in batch\n",
        "            if \"original_source\" in batch and \"original_target\" in batch:\n",
        "                orig_sources = batch[\"original_source\"]\n",
        "                orig_targets = batch[\"original_target\"]\n",
        "            else:\n",
        "                # Fallback to decoding from input_ids/labels\n",
        "                orig_sources = [\n",
        "                    self.tokenizer.decode(ids, skip_special_tokens=True).replace(SRC_PREFIX, \"\")\n",
        "                    for ids in batch[\"input_ids\"]\n",
        "                ]\n",
        "                orig_targets = [\n",
        "                    self.tokenizer.decode(ids[ids != -100], skip_special_tokens=True)\n",
        "                    for ids in batch[\"labels\"]\n",
        "                ]\n",
        "\n",
        "            # Prepare batch for cycle loss\n",
        "            batch_for_cycle = {\n",
        "                \"original_source\": orig_sources,\n",
        "                \"original_target\": orig_targets\n",
        "            }\n",
        "\n",
        "            # Compute cycle loss\n",
        "            cycle_score = compute_cycle_loss(model, self.tokenizer, batch_for_cycle, self.args.device)\n",
        "            cycle_scores.append(cycle_score)\n",
        "\n",
        "        # Compute average cycle score\n",
        "        avg_cycle_score = sum(cycle_scores) / len(cycle_scores) if cycle_scores else 0\n",
        "        metrics[\"cycle_consistency\"] = avg_cycle_score\n",
        "        self.cycle_losses.append(avg_cycle_score)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    global tokenizer  # Make tokenizer available to compute_metrics function\n",
        "\n",
        "    # Load the data\n",
        "    print(f\"Loading data from {TSV_FILE_PATH}...\")\n",
        "    df = load_dataset(TSV_FILE_PATH)\n",
        "    print(f\"Loaded {len(df)} translation pairs.\")\n",
        "\n",
        "    # Split data into train and validation sets\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "    print(f\"Training set: {len(train_df)} examples\")\n",
        "    print(f\"Validation set: {len(val_df)} examples\")\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"Loading T5 tokenizer and model...\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(\n",
        "        train_df, tokenizer, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH, SRC_PREFIX\n",
        "    )\n",
        "    val_dataset = TranslationDataset(\n",
        "        val_df, tokenizer, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH, SRC_PREFIX\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Generate a unique run name\n",
        "    run_name = f\"t5-khasi-english-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        run_name=run_name,\n",
        "        eval_strategy=\"epoch\",  # Use the updated parameter name\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        predict_with_generate=True,\n",
        "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"bleu\",\n",
        "        greater_is_better=True,\n",
        "        push_to_hub=False,\n",
        "        report_to=\"none\",  # Disable all integrations (wandb, tensorboard, etc.)\n",
        "        disable_tqdm=False,  # Show progress bars\n",
        "    )\n",
        "\n",
        "    # Trainer with Cycle Consistency\n",
        "    trainer = CycleConsistencyTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating model...\")\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"Final evaluation metrics: {metrics}\")\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    print(f\"Saving model to {OUTPUT_DIR}...\")\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    # Plot cycle consistency loss over time\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(range(1, len(trainer.cycle_losses) + 1), trainer.cycle_losses)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Cycle Consistency Score (BLEU)')\n",
        "        plt.title('Cycle Consistency Over Training')\n",
        "        plt.savefig(f\"{OUTPUT_DIR}/cycle_consistency.png\")\n",
        "        print(f\"Cycle consistency plot saved to {OUTPUT_DIR}/cycle_consistency.png\")\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not available, skipping cycle consistency plot.\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afI5BYA7nViX",
        "outputId": "da180eb6-f00c-4b58-856e-2513b9891143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
